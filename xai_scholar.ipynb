{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import string\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "df = pd.read_json('xai_scholar_metadata_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('paperId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load communities\n",
    "comm_df = pd.read_json(f'communities/final_oslom_{level}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_df.columns = [col-2000 for col in comm_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community lables (TFIDF)\n",
    "with open(f'topic_labels_oslom_{level}.json', 'r') as f_in:\n",
    "    label_dict = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community labels (CTD)\n",
    "with open(f'topic_labels_oslom_{level}_reweighted.json', 'r') as f_in:\n",
    "    label_dict_reweighted = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_sizes = {}\n",
    "for year in range(24):\n",
    "    community_sizes[year] = comm_df[year].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "for year in tqdm(range(24)):\n",
    "    graphs[year] = nx.read_gexf(f'citation_graphs/{2000+year}.0.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate community interaction networks\n",
    "community_graphs_normed = {}\n",
    "for year in tqdm(range(24)):\n",
    "\n",
    "    G = graphs[year].copy()\n",
    "    communities = list(comm_df[year].explode().replace(0,np.nan).reset_index().groupby(year).agg(list)['index'])\n",
    "    \n",
    "    edge_weights = {}\n",
    "    for id_u, u_community in enumerate(communities):\n",
    "        for id_v, v_community in enumerate(communities):\n",
    "            weight = sum(1 for u in u_community for v in v_community if G.has_edge(u, v))\n",
    "            denom = len(u_community) * len(v_community)\n",
    "            edge_weights[id_u+1,id_v+1] = weight / denom\n",
    "    \n",
    "    # Create the block model graph\n",
    "    block_model_graph = nx.Graph()\n",
    "    block_model_graph.add_nodes_from([community+1 for community in range(len(communities))])\n",
    "    block_model_graph.add_weighted_edges_from([(u, v, weight) for (u, v), weight in edge_weights.items()])\n",
    "    \n",
    "    nx.set_node_attributes(block_model_graph, community_sizes[year], 'size')\n",
    "    \n",
    "    community_labels = {int(k) : \"_\".join(label_dict[str(year)][k][:3]) for k in label_dict[str(year)]}\n",
    "    \n",
    "    nx.set_node_attributes(block_model_graph, community_labels, 'label')\n",
    "    \n",
    "    community_fields = {}\n",
    "    for i, comm in enumerate(communities):\n",
    "        fields = sorted(df.loc[comm]['fields'].explode().value_counts().head(1).index)\n",
    "        community_fields[i+1] = \"_\".join(fields)\n",
    "    \n",
    "    nx.set_node_attributes(block_model_graph, community_fields, 'fields')\n",
    "\n",
    "    community_graphs_normed[year] = block_model_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate community interaction newtworks \n",
    "community_graphs = {}\n",
    "for year in tqdm(range(24)):\n",
    "\n",
    "    G = graphs[year].copy()\n",
    "    communities = list(comm_df[year].explode().replace(0,np.nan).reset_index().groupby(year).agg(list)['index'])\n",
    "    \n",
    "    edge_weights = {}\n",
    "    for id_u, u_community in enumerate(communities):\n",
    "        for id_v, v_community in enumerate(communities):\n",
    "            weight = sum(1 for u in u_community for v in v_community if G.has_edge(u, v))\n",
    "            # denom = len(u_community) * len(v_community)\n",
    "            if (weight > 1) & (id_u != id_v): \n",
    "                edge_weights[id_u+1,id_v+1] = weight # / denom\n",
    "    \n",
    "    # Create the block model graph\n",
    "    block_model_graph = nx.Graph()\n",
    "    block_model_graph.add_nodes_from([community+1 for community in range(len(communities))])\n",
    "    block_model_graph.add_weighted_edges_from([(u, v, weight) for (u, v), weight in edge_weights.items()])\n",
    "    \n",
    "    nx.set_node_attributes(block_model_graph, community_sizes[year], 'size')\n",
    "    \n",
    "    community_labels = {int(k) : \"_\".join(label_dict[str(year)][k][:3]) for k in label_dict[str(year)]}\n",
    "    \n",
    "    nx.set_node_attributes(block_model_graph, community_labels, 'label')\n",
    "    \n",
    "    community_fields = {}\n",
    "    for i, comm in enumerate(communities):\n",
    "        fields = sorted(df.loc[comm]['fields'].explode().value_counts().head(1).index)\n",
    "        community_fields[i+1] = \"_\".join(fields)\n",
    "    \n",
    "    nx.set_node_attributes(block_model_graph, community_fields, 'fields')\n",
    "\n",
    "    community_graphs[year] = block_model_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dict = {d:{0:d} for d in comm_df[0].explode().replace(0,np.nan).dropna().unique()}\n",
    "scores = {d:{0:d} for d in comm_df[0].explode().replace(0,np.nan).dropna().unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comm_lookup(year, comm_id):\n",
    "    return set(comm_df[year].explode()[comm_df[year].explode() == comm_id].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_counter = max(list(d_dict))\n",
    "for y in range(1,24):\n",
    "    comms = comm_df[y]\n",
    "    for c in comms.explode().replace(0,np.nan).dropna().unique():\n",
    "        match_found = False\n",
    "        c_set = comm_lookup(y,c)\n",
    "        for d in d_dict.copy():\n",
    "            # D = comm_df[y-1]\n",
    "            # d_set = set(D[D==d].index)\n",
    "            front_year = max(d_dict[d])\n",
    "            front_name = d_dict[d][front_year]\n",
    "            d_set = comm_lookup(front_year,front_name)\n",
    "            j_sim = len(c_set.intersection(d_set)) / len(c_set.union(d_set))\n",
    "            if j_sim >= t:\n",
    "                match_found = True\n",
    "                if not y in d_dict[d]:\n",
    "                    d_dict[d][y] = c\n",
    "                    scores[d][y] = j_sim\n",
    "                else: \n",
    "                    d_counter+=1\n",
    "                    d_dict[d_counter] = d_dict[d]\n",
    "                    d_dict[d_counter][y] = c \n",
    "                    scores[d_counter][y] = j_sim\n",
    "        if not match_found:\n",
    "            d_counter+=1\n",
    "            d_dict[d_counter] = {y:c}\n",
    "            scores[d_counter] = {y:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_df = pd.DataFrame(d_dict).T\n",
    "scores_df = pd.DataFrame(scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_df = dyn_comm_df[range(24)]\n",
    "dyn_comm_df[dyn_comm_df.notna().sum(axis = 1) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_df = dyn_comm_df[dyn_comm_df.notna().sum(axis = 1) > 1][range(24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/scibert.json','r') as infile:\n",
    "    scibert = json.load(infile)\n",
    "\n",
    "scibert = pd.Series(scibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_df = pd.DataFrame()\n",
    "for year in range(24):\n",
    "    size_df[year] = dyn_comm_df[year].map(lambda x: community_sizes[year][x], na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_densities = {}\n",
    "for year in range(24):\n",
    "    comms = comm_df[year].reset_index().groupby(year).index.agg(list)\n",
    "    dens = {}\n",
    "    for comm_id, comm in comms.items(): \n",
    "        dens[comm_id] = nx.density(graphs[year].subgraph(comm))\n",
    "    community_densities[year] = dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_df = pd.DataFrame()\n",
    "for year in range(24):\n",
    "    dens_df[year] = dyn_comm_df[year].map(lambda x: community_densities[year][x], na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_disparity = {}\n",
    "for year in range(24):\n",
    "    comms = comm_df[year].reset_index().groupby(year).index.agg(list)\n",
    "    dens = {}\n",
    "    for comm_id, papers in comms.items(): \n",
    "        embs = scibert[papers]\n",
    "        X = np.vstack(embs)\n",
    "        m = X.mean(axis=0).reshape(1,-1)        \n",
    "        dens[comm_id] = cosine_similarity(X,m).mean()\n",
    "    community_disparity[year] = dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df = pd.DataFrame()\n",
    "for year in range(24):\n",
    "    disp_df[year] = dyn_comm_df[year].map(lambda x: community_disparity[year][x], na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "for year in range(1,24):\n",
    "    counts = dyn_comm_df[year].value_counts()\n",
    "    cands = list(counts[counts > 1].index)\n",
    "    # print('\\n')\n",
    "    for cand in cands:\n",
    "        dyn_comms = dyn_comm_df[dyn_comm_df[year] == cand]\n",
    "        dyn_comm_1 = dyn_comms.iloc[0]\n",
    "        dyn_comm_1_id = dyn_comm_1.name\n",
    "        for dyn_comm_2_id, dyn_comm_2 in dyn_comms.iloc[1:].iterrows():\n",
    "            if not dyn_comm_1[range(year)].eq(dyn_comm_2[range(year)]).any():\n",
    "                # print(f'comm {dyn_comm_1_id} and comm {dyn_comm_2_id} merge in {2000+year}')\n",
    "                merges.append({'comm1':dyn_comm_1_id,'comm2':dyn_comm_2_id,'year':year})\n",
    "            elif not dyn_comm_1[range(year,24)].dropna().eq(dyn_comm_2[range(year,24)].dropna(),fill_value = 0).all():\n",
    "                print(f'comm {dyn_comm_1_id} and comm {dyn_comm_2_id} SPLIT after {2000+year}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_communities(year1, year2, comm1, comm2):\n",
    "    \n",
    "    papers_1 = list(comm_lookup(year1, comm1))\n",
    "    papers_2 = list(comm_lookup(year2, comm2))\n",
    "    X = np.vstack(scibert[papers_1])\n",
    "    Y = np.vstack(scibert[papers_2])\n",
    "    return cosine_similarity(X,Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comm_interactions(year1, year2, comm1, comm2):\n",
    "    \n",
    "    comm1, comm2 = comm_lookup(year1, comm1), comm_lookup(year2, comm2)\n",
    "    G = graphs[max(year1,year2)].copy()\n",
    "    weight = sum(1 for u in comm1 for v in comm2 if G.has_edge(u, v))\n",
    "    denom = len(comm1) * len(comm2)\n",
    "    return weight / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_by_year = {}\n",
    "for year in range(1,24):\n",
    "    other_communities = list(dyn_comm_df[year].dropna())\n",
    "    sims = []\n",
    "    for comms in combinations(other_communities, r = 2):\n",
    "        sims.append(compare_communities(year, year, *comms)) \n",
    "    sim_by_year[year] = sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_by_year = {}\n",
    "for year in tqdm(range(1,24)):\n",
    "    other_communities = list(dyn_comm_df[year].dropna())\n",
    "    G = community_graphs[year].copy()\n",
    "    X = nx.to_numpy_array(G)\n",
    "    X = np.tril(X, k = -1)\n",
    "    int_by_year[year] = X[X > 0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "pd.concat([pd.Series(int_by_year[year]) for year in int_by_year], axis = 1).boxplot(ax = ax)\n",
    "for merge in merges:\n",
    "    ax.scatter(merge['year'],merge['interactions'], color = 'red', marker = '+', zorder = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "pd.concat([pd.Series(sim_by_year[year]) for year in sim_by_year], axis = 1).boxplot(ax = ax)\n",
    "for merge in merges:\n",
    "    ax.scatter(merge['year'],merge['sim'], color = 'red', marker = '+', zorder = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for merge in merges:\n",
    "    year = merge['year']\n",
    "    \n",
    "    x = merge['sim']\n",
    "    data = sim_by_year[year]\n",
    "    std = np.std(data)\n",
    "    mean = np.mean(data)\n",
    "    merge['sim_z'] = (x - mean) / std\n",
    "    \n",
    "    x = merge['interactions']\n",
    "    data = int_by_year[year]\n",
    "    data = np.log(data)\n",
    "    std = np.std(data)\n",
    "    mean = np.mean(data)\n",
    "    merge['int_z'] = (np.log(x) - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_mean_disp(comm1, comm2, year):\n",
    "    disps = [] \n",
    "    for comm in [comm1,comm2]:\n",
    "        d = dyn_comm_df.loc[comm][range(int(year))].dropna().to_dict()\n",
    "        y = max(d)\n",
    "        c = d[y]\n",
    "        disps.append(community_disparity[y][c])\n",
    "    return len(disps) / sum([1/d for d in disps]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.DataFrame(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['coherence'] = merge_df.apply(lambda row: merge_mean_disp(row['comm1'],row['comm2'],row['year']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['sim_adj'] = merge_df['sim']*merge_df['coherence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['sim_adj_z'] = (merge_df['sim_adj'] - merge_df['sim_adj'].mean()) / merge_df['sim_adj'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, merge in merge_df[(merge_df['year1'] == merge_df['year2']) & (merge_df['sim_adj_z']<0)].sort_values('sim_adj_z').iterrows():\n",
    "    sub_df = dyn_comm_df.loc[[merge['comm1'],merge['comm2']]]\n",
    "    for i in range(2):\n",
    "        d = sub_df.iloc[i][range(int(merge['year']))].dropna().to_dict()\n",
    "        year = max(d)\n",
    "        comm = d[year]\n",
    "        labels = label_dict[str(int(year))][str(int(comm))]\n",
    "        disp = community_disparity[year][comm]\n",
    "        dens = community_densities[year][comm]\n",
    "        \n",
    "        print(f'{dens:.2f},{disp:.2f}:{labels}')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = list(dyn_comm_df.loc[1].dropna().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_stability(dyn_comm):\n",
    "    comms = list(dyn_comm.dropna().items())\n",
    "    content_sim = []\n",
    "    membership_sim = []\n",
    "    sizes = []\n",
    "    fields = []\n",
    "    for i in range(len(comms) - 1):\n",
    "        (year1, comm1), (year2, comm2) = comms[i], comms[i+1]\n",
    "        content_sim.append(compare_communities(year1, year2, comm1, comm2))\n",
    "        c_set_1 = comm_lookup(year1,comm1)\n",
    "        c_set_2 = comm_lookup(year2,comm2)\n",
    "        membership_sim.append(len(c_set_1.intersection(c_set_2)) / len(c_set_1.union(c_set_2)))\n",
    "        sizes.append(len(c_set_1))\n",
    "        field = df.loc[list(c_set_1)].fields.explode().value_counts()\n",
    "        field = list(field[field>0.5].index)\n",
    "        fields.append(field)\n",
    "    field = df.loc[list(c_set_2)].fields.explode().value_counts()\n",
    "    field = list(field[field>0.5].index)\n",
    "    fields.append(field)\n",
    "    sizes.append(len(c_set_2))\n",
    "    return pd.Series({'membership_sim':membership_sim,'content_sim':content_sim,'lifespan':len(comms),'sizes':sizes,'fields':fields})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_stats = dyn_comm_df.apply(community_stability,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_stats['field'] = dyn_comm_stats['fields'].map(lambda x: pd.Series(x).explode().value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_stats['membership_stability'] = dyn_comm_stats['membership_sim'].map(np.mean)\n",
    "dyn_comm_stats['content_stability'] = dyn_comm_stats['content_sim'].map(np.mean)\n",
    "dyn_comm_stats['size_mean'] = dyn_comm_stats['sizes'].map(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = dyn_comm_stats.fields.explode().map(lambda x:x[0]).unique()\n",
    "field_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Lifecycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = ['Mathematics', 'Computer Science', 'Engineering', 'Physics', 'Psychology', 'Philosophy', 'Medicine', 'Biology', 'Business', 'Economics', 'Environmental Science']\n",
    "\n",
    "field_labels = dict(zip(field_list,range(len(field_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_fields = []\n",
    "for comm in dyn_comm_df.index:\n",
    "    fields = dyn_comm_stats.loc[comm]['fields']\n",
    "    d = dyn_comm_df.loc[comm].dropna().to_dict()\n",
    "    for i,k in enumerate(d):\n",
    "        d[k] = field_labels[fields[i][0]]\n",
    "    for i in range(24):\n",
    "        if not i in d:\n",
    "            d[i] = np.nan\n",
    "    d['comm'] = comm\n",
    "    dyn_comm_fields.append(d)\n",
    "dyn_comm_fields = pd.DataFrame(dyn_comm_fields).set_index('comm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_comms_mask = (dyn_comm_fields[range(16)].notna().sum(axis=1)==0)&(dyn_comm_fields[range(16,24)].notna().sum(axis=1)==6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### n = len(field_labels)\n",
    "cmap = sns.color_palette(\"Paired\", n) \n",
    "# size_min, size_max = [0.7,0.8]\n",
    "fig, ax = plt.subplots(1,1,figsize = (8,8))\n",
    "sns.heatmap(dyn_comm_fields[recent_comms_mask], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=ax, vmin=0, vmax=10)\n",
    "ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "ax.set_xticklabels([i+2000 if i%2==0 else '' for i in range(24)])\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "ax.set_ylabel('')\n",
    "ax.set_yticks([])\n",
    "colorbar = ax.collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin \n",
    "colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "colorbar.set_ticklabels(list(field_labels))                                          \n",
    "# fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Pairwise Community Interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {}\n",
    "s = {}\n",
    "for i in range(23):\n",
    "    years = (i, i+1)\n",
    "    pairs = dyn_comm_df[list(years)].dropna()\n",
    "    for comm1, comm2 in tqdm(list(combinations(pairs.itertuples(), r = 2)), postfix = f'{years}'):\n",
    "        p2 = community_graphs_normed[years[1]][comm1[2]][comm2[2]]['weight']\n",
    "        p1 = community_graphs_normed[years[0]][comm1[1]][comm2[1]]['weight'] \n",
    "        idx = (comm1[0],comm2[0]) \n",
    "        # if not comm1[1] == comm2[1]:\n",
    "        if not idx in p:\n",
    "            p[idx] = {}\n",
    "        p[idx][years[0]] = p1\n",
    "        p[idx][years[1]] = p2\n",
    "        if not idx in s:\n",
    "            s[idx] = {}\n",
    "        s[idx][years[0]] = compare_communities(years[0],years[0],comm1[1],comm2[1])\n",
    "        s[idx][years[1]] = compare_communities(years[1],years[1],comm1[2],comm2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_s = pd.Series(p)\n",
    "sim_s = pd.Series(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_s = pd.DataFrame(int_s)\n",
    "int_s['len'] = int_s[0].map(len)\n",
    "sim_s = pd.DataFrame(sim_s)\n",
    "sim_s['len'] = sim_s[0].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df = int_s[0].apply(pd.Series)\n",
    "sim_df = sim_s[0].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_lookup = {}\n",
    "for year in range(24):\n",
    "    bet_lookup[year] = dict(nx.betweenness_centrality(community_graphs[year]))\n",
    "\n",
    "bet_df = pd.DataFrame(columns=range(24))\n",
    "for label, row in dyn_comm_df.iterrows():\n",
    "    cent = pd.Series()\n",
    "    for year, id_c in row.dropna().items():\n",
    "        cent[year] = bet_lookup[year][id_c]\n",
    "    bet_df.loc[label] = cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_lookup = {}\n",
    "for year in range(24):\n",
    "    deg_lookup[year] = dict(nx.degree(community_graphs[year]))\n",
    "\n",
    "deg_df = pd.DataFrame(columns=range(24))\n",
    "for label, row in dyn_comm_df.iterrows():\n",
    "    cent = pd.Series()\n",
    "    for year, id_c in row.dropna().items():\n",
    "        cent[year] = deg_lookup[year][id_c]\n",
    "    deg_df.loc[label] = cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].value_counts()[range(2000,2024)].plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question 1: \n",
    "- Identify the foundational topics in the literature:\n",
    "    Longlived, coherent communities with sustained centrality\n",
    "- Identify contemporary topics as large, recent communities\n",
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(field_labels)\n",
    "cmap = sns.color_palette(\"Paired\", n) \n",
    "# size_min, size_max = [0.7,0.8]\n",
    "fig, ax = plt.subplots(1,1,figsize = (8,8))\n",
    "sns.heatmap(dyn_comm_fields[(lifespan>=11)], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=ax, vmin=0, vmax=10)\n",
    "# ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "ax.set_xticklabels([i+2000 if i%2==0 else '' for i in range(24)])\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "ax.set_ylabel('')\n",
    "# ax.set_yticks([])\n",
    "colorbar = ax.collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin \n",
    "colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "colorbar.set_ticklabels(list(field_labels))                                          \n",
    "# fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = ['data', 'model', 'network', 'disentanglement', 'models']\n",
    "def annotate_dynamic_community(comm, n_chars=50, n_terms=3, y=24, reweighted=False):\n",
    "    if reweighted:\n",
    "        labels = label_dict_reweighted\n",
    "    else:\n",
    "        labels = label_dict\n",
    "    comm = float(comm)\n",
    "    term_series = dyn_comm_df.loc[comm][range(y)].dropna().T.reset_index().apply(lambda x: labels[str(int(x['index']))][str(int(x[comm]))], axis = 1)\n",
    "    terms = term_series.explode()\n",
    "    terms = terms[terms.apply(lambda x: x not in stop)]\n",
    "    term_counts = terms.value_counts()\n",
    "    top_terms = term_counts.iloc[:10].index\n",
    "    stop_terms = []\n",
    "    for term in top_terms:\n",
    "        # stop_terms.append(term+'s')\n",
    "        stop_terms.append(term[:-1])\n",
    "        split_term = term.split(' ')\n",
    "        if len(split_term) > 1:\n",
    "            stop_terms.extend(split_term)\n",
    "            # stop_terms.extend([t+'s' for t in split_term])\n",
    "            stop_terms.extend([t[:-1] for t in split_term])\n",
    "    top_terms = [term for term in top_terms if term not in stop_terms]\n",
    "    # print(stop_terms)\n",
    "    label_string = '/'.join(top_terms)\n",
    "    n = len(top_terms)\n",
    "    while (len(label_string) > n_chars) or (n>n_terms):\n",
    "        n-=1\n",
    "        label_string = '/'.join(top_terms[:n])\n",
    "    return label_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_community(comm, n=5, y=24, reweighted=False):\n",
    "    if reweighted:\n",
    "        labels = label_dict_reweighted\n",
    "    else:\n",
    "        labels = label_dict\n",
    "    terms = pd.Series(labels['23'][str(int(comm))])\n",
    "    terms = terms[terms.apply(lambda x: x not in stop)]\n",
    "    term_counts = terms.value_counts()\n",
    "    top_terms = term_counts.iloc[:10].index\n",
    "    stop_terms = []\n",
    "    for term in top_terms:\n",
    "        # stop_terms.append(term+'s')\n",
    "        stop_terms.append(term[:-1])\n",
    "        split_term = term.split(' ')\n",
    "        if len(split_term) > 1:\n",
    "            stop_terms.extend(split_term)\n",
    "            # stop_terms.extend([t+'s' for t in split_term])\n",
    "            stop_terms.extend([t[:-1] for t in split_term])\n",
    "    top_terms = [term for term in top_terms if term not in stop_terms][:n]\n",
    "    # print(stop_terms)\n",
    "    return '/'.join(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_central = bet_df[lifespan>10][range(10)].fillna(0).mean(axis = 1).sort_values(ascending = False).dropna().head(8)\n",
    "most_central.index.map(lambda x: print(x,annotate_dynamic_community(x),most_central[x]))\n",
    "least_central = bet_df[lifespan>12][range(10)].mean(axis = 1).sort_values().dropna().head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### n = len(field_labels)\n",
    "cmap = sns.color_palette(\"Paired\", n) \n",
    "# size_min, size_max = [0.7,0.8]\n",
    "fig, ax = plt.subplots(1,1,figsize = (8,8))\n",
    "sns.heatmap(dyn_comm_fields.loc[most_central.index], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=ax, vmin=0, vmax=10)\n",
    "# ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "ax.set_xticklabels([i+2000 if i%4==0 else '' for i in range(24)])\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "ax.set_yticklabels([annotate_dynamic_community(i,n_chars=35) for i in most_central.index])\n",
    "# ax.set_yticks([])\n",
    "colorbar = ax.collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin \n",
    "colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "colorbar.set_ticklabels(list(field_labels))                                          \n",
    "# fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.05)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del field_labels['Environmental Science']\n",
    "n = len(field_labels)\n",
    "cmap = sns.color_palette(\"Paired\", n) \n",
    "# size_min, size_max = [0.7,0.8]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(11, 4),gridspec_kw={'width_ratios': [1, 0.65, 1, 0.04]})\n",
    "sns.heatmap(dyn_comm_fields.loc[most_central.index], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=axs[0], vmin=0, vmax=10, cbar=False)\n",
    "# ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "axs[0].set_yticklabels([annotate_dynamic_community(i,n_chars=35) for i in most_central.index])\n",
    "# ax.set_yticks([])\n",
    "\n",
    "sns.heatmap(dyn_comm_fields.loc[least_central.index], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=axs[2], vmin=0, vmax=10, cbar_ax=axs[3])\n",
    "# ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "x_labels = [(2*i)+2000 if i%4==0 else '' for i in range(12)]\n",
    "axs[2].set_xticklabels(x_labels)\n",
    "axs[0].set_xticklabels(x_labels)\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "axs[2].set_yticklabels([annotate_dynamic_community(i,n_chars=30) for i in least_central.index])\n",
    "# ax.set_yticks([])\n",
    "colorbar = axs[2].collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin \n",
    "colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "colorbar.set_ticklabels(list(field_labels))                                          \n",
    "# fig.tight_layout()\n",
    "axs[0].set_yticks(axs[0].get_yticks(), axs[0].get_yticklabels(), rotation=0)\n",
    "axs[2].set_yticks(axs[2].get_yticks(), axs[2].get_yticklabels(), rotation=0)\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.05)\n",
    "plt.yticks(rotation=0)\n",
    "axs[0].set_title('High Centrality')\n",
    "axs[2].set_title('Low Centrality')\n",
    "axs[1].set_axis_off()\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(wspace = 1)\n",
    "plt.savefig('figures/high_centrality_low_centrality.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_most_central = bet_df[bet_df[23].notna()][[21,22,23]].mean(axis = 1).sort_values(ascending=False).head(7)\n",
    "modern_most_central.index.map(lambda x: print(x,annotate_dynamic_community(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del field_labels['Environmental Science']\n",
    "n = len(field_labels)\n",
    "cmap = sns.color_palette(\"Paired\", n) \n",
    "# size_min, size_max = [0.7,0.8]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4),gridspec_kw={'width_ratios': [1, 0.6, 1, 0.04]})\n",
    "sns.heatmap(dyn_comm_fields.loc[most_central.index], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=axs[0], vmin=0, vmax=10, cbar=False)\n",
    "# ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "axs[0].set_yticklabels([annotate_dynamic_community(i,n_chars=30) for i in most_central.index])\n",
    "# ax.set_yticks([])\n",
    "\n",
    "sns.heatmap(dyn_comm_fields.loc[modern_most_central.index], cmap=cmap, linecolor='white', linewidths=0.1, \n",
    "            ax=axs[2], vmin=0, vmax=10, cbar_ax=axs[3])\n",
    "# ax.set_yticklabels([])\n",
    "str_intervals = str(q).replace(\"(\",\"\").replace(\"]\", \"\").split(\", \")\n",
    "x_labels = [(2*i)+2000 if i%4==0 else '' for i in range(12)]\n",
    "axs[2].set_xticklabels(x_labels)\n",
    "axs[0].set_xticklabels(x_labels)\n",
    "# ax.xticks(rotation=30, ha='center')\n",
    "axs[2].set_yticklabels([annotate_dynamic_community(i,n_chars=35) for i in modern_most_central.index])\n",
    "# ax.set_yticks([])\n",
    "colorbar = axs[2].collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin \n",
    "colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "colorbar.set_ticklabels(list(field_labels))                                          \n",
    "# fig.tight_layout()\n",
    "axs[0].set_yticks(axs[0].get_yticks(), axs[0].get_yticklabels(), rotation=0)\n",
    "axs[2].set_yticks(axs[2].get_yticks(), axs[2].get_yticklabels(), rotation=0)\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.05)\n",
    "plt.yticks(rotation=0)\n",
    "axs[0].set_title('Foundation Topics')\n",
    "axs[2].set_title('Recent Central Topics')\n",
    "axs[1].set_axis_off()\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(wspace = 1)\n",
    "plt.savefig('figures/central_topics.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_year(year,comm):\n",
    "    papers = comm_lookup(year,comm)\n",
    "    return pd.Series([df['year'][paper] for paper in papers]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(community_year(23,front)) for front in fronts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_stats['year'] = dyn_comm_df[23].map(lambda x: int(community_year(23,x)), na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = dyn_comm_stats['content_stability'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (dyn_comm_stats['size_mean']>50)\n",
    "mask = mask&(dyn_comm_stats['content_stability']>threshold)\n",
    "mask = mask&(dyn_comm_stats['year']>=2017)\n",
    "mask = mask&(bet_df[23].notna())\n",
    "# mask = mask&(prop_s>0.1)\n",
    "mask = mask&(lifespan>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_comms = dyn_comm_stats[mask].sort_values('year', ascending=False).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_fronts = dyn_comm_df.loc[recent_comms.index][23].map(int)#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fronts, most_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 35\n",
    "X = nx.to_numpy_array(community_graphs_normed[23]).T[recent_fronts][:, fronts] * 100\n",
    "foundation_order = X.mean(axis=0).argsort()\n",
    "sorted_X = X[X.mean(axis=1).argsort()][:, foundation_order]\n",
    "\n",
    "# Compute heatmap data for the second heatmap (X_modern)\n",
    "X_modern = nx.to_numpy_array(community_graphs_normed[23]).T[recent_fronts][:, modern_fronts] * 100\n",
    "modern_order = X_modern.mean(axis=0).argsort()\n",
    "sorted_X_modern = X_modern[X.mean(axis=1).argsort()][:, X_modern.mean(axis=0).argsort()]\n",
    "\n",
    "# Create subplots to display heatmaps side by side\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 8),gridspec_kw={'width_ratios': [1, 1, 0.05]})\n",
    "\n",
    "# Plot the first heatmap (sorted_X) on the first subplot (axs[0])\n",
    "sns.heatmap(sorted_X, annot=True, vmax=2, cbar=False, linecolor='white', linewidths=0.1, cmap='crest',\n",
    "            xticklabels=[annotate_dynamic_community(front, n_chars=n_chars) for front in most_central.index[foundation_order]],\n",
    "            yticklabels=[annotate_dynamic_community(front, n_chars=n_chars) for front in recent_comms.index[X.mean(axis=1).argsort()]],\n",
    "            ax=axs[0])\n",
    "axs[0].set_title('Foundation Topics')\n",
    "\n",
    "# Plot the second heatmap (sorted_X_modern) on the second subplot (axs[1])\n",
    "sns.heatmap(sorted_X_modern, annot=True, vmax=2, cbar_ax=axs[2], linecolor='white', linewidths=0.1, cmap='crest',\n",
    "            xticklabels=[annotate_dynamic_community(front, n_chars=n_chars) for front in modern_most_central.index[modern_order]],\n",
    "            yticklabels=[annotate_dynamic_community(front, n_chars=n_chars) for front in recent_comms.index[X.mean(axis=1).argsort()]],\n",
    "            ax=axs[1])\n",
    "axs[1].set_title('Recent Central Topics')\n",
    "axs[1].set_yticklabels([])\n",
    "axs[0].set_xticks(axs[0].get_xticks(), axs[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axs[1].set_xticks(axs[1].get_xticks(), axs[1].get_xticklabels(), rotation=45, ha='right')\n",
    "# Adjust layout to prevent overlap of subplots\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "axs[0].set_title('Foundation Topics', fontsize=16)\n",
    "axs[1].set_title('Recent Central Topics', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('figures/topic_interactions.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 3\n",
    "X = nx.to_numpy_array(community_graphs_normed[23]).T[recent_fronts][:, fronts] * 100\n",
    "sorted_X = X[X.mean(axis=1).argsort()][:, foundation_order]\n",
    "\n",
    "# Compute heatmap data for the second heatmap (X_modern)\n",
    "X_modern = nx.to_numpy_array(community_graphs_normed[23]).T[recent_fronts][:, modern_fronts] * 100\n",
    "sorted_X_modern = X_modern[X.mean(axis=1).argsort()][:, modern_order]\n",
    "\n",
    "# Create subplots to display heatmaps side by side\n",
    "fig, axs = plt.subplots(4, 3, figsize=(10, 8),gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "ax_cbar = fig.add_subplot(1,40,40)\n",
    "\n",
    "ls = [np.array([472,332,258]), np.array([602,451,331]),  np.array([543,262,266]), np.array([458,493,301])]\n",
    "topic_subsets = ['Fairness\\n', 'Natural Language \\n Processing', 'Computer Vision\\n', 'Adversarial ML\\n']\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    l = ls[i]\n",
    "    l_x = np.array([recent_fronts[l_x_i] for l_x_i in ls[i]])\n",
    "    X = nx.to_numpy_array(community_graphs_normed[23]).T[l_x][:,fronts]*100\n",
    "    sns.heatmap(X[:,foundation_order], annot = True,\n",
    "                vmax=2, ax=axs[i][0], cbar = False, linecolor='white', linewidths=0.1, cmap='crest',\n",
    "                xticklabels=[annotate_dynamic_community(front, n_chars=35) for front in most_central.index[foundation_order]],\n",
    "                yticklabels=[annotate_dynamic_community(f, n_chars=35) for f in l[X.mean(axis=1).argsort()]])\n",
    "    if i < (len(axs)-1):\n",
    "        axs[i][0].set_xticklabels([])\n",
    "        axs[i][0].set_xticks([])\n",
    "    else:\n",
    "        axs[i][0].set_xticks(axs[i][0].get_xticks(), axs[i][0].get_xticklabels(), rotation=45, ha='right')\n",
    "    # axs[i][0].set_ylabel(topic_subsets[i],fontsize = 12)\n",
    "    axs[i][0].yaxis.set_label_coords(-0.6,0.55)\n",
    "        \n",
    "    X_modern = nx.to_numpy_array(community_graphs_normed[23]).T[l_x][:, modern_fronts] * 100\n",
    "    sns.heatmap(X_modern[:,modern_order], annot = True,\n",
    "                vmax=2, ax=axs[i][1], cbar_ax = ax_cbar, linecolor='white', linewidths=0.1, cmap='crest',\n",
    "                xticklabels=[annotate_dynamic_community(front, n_chars=35) for front in modern_most_central.index[modern_order]],\n",
    "                yticklabels=[annotate_dynamic_community(f, n_chars=35) for f in l[X.mean(axis=1).argsort()]])\n",
    "    if i < (len(axs)-1):\n",
    "        axs[i][1].set_xticklabels([])\n",
    "        axs[i][1].set_xticks([])\n",
    "    else:\n",
    "        axs[i][1].set_xticks(axs[i][1].get_xticks(), axs[i][1].get_xticklabels(), rotation=45, ha='right')\n",
    "    axs[i][1].set_yticklabels([])\n",
    "    axs[i][2].set_axis_off()\n",
    "# Adjust layout to prevent overlap of subplots\n",
    "# plt.tight_layout()\n",
    "\n",
    "axs[0][0].set_title('Foundation Topics', fontsize=16)\n",
    "axs[0][1].set_title('Recent Central Topics', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('figures/topic_interactions_sub.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question 2:\n",
    "\n",
    "-Model the relationship between content similarity, second-order network proximity and citations/community interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_df[23].dropna().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = pd.concat([int_df[23].dropna(),sim_df[23].dropna()], axis = 1)\n",
    "rel_df.columns = ['interactions', 'content_sim']\n",
    "rel_df['interactions_log'] = rel_df['interactions'].map(lambda x: np.log(10*(x+1)))\n",
    "rel_df['content_sim_std'] = (rel_df['content_sim']-rel_df['content_sim'].mean()) / rel_df['content_sim'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TweedieRegressor, LinearRegression, GammaRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nx.to_numpy_array(community_graphs_normed[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_neighborhoods(comm1,comm2):\n",
    "    f1, f2 = int(dyn_comm_df[23].loc[comm1]), int(dyn_comm_df[23].loc[comm2])\n",
    "    return cosine_similarity([X[f2]],[X[f1]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df['neighborhood_sim'] = rel_df.index.map(lambda comms: compare_neighborhoods(*comms))\n",
    "rel_df['neighborhood_sim_std'] = (rel_df['neighborhood_sim'] - rel_df['neighborhood_sim'].mean()) / rel_df['neighborhood_sim'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GammaRegressor()\n",
    "X,y = rel_df[['content_sim_std','neighborhood_sim_std']],rel_df['interactions_log']\n",
    "np.mean(-cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)\n",
    "rel_df['pred'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df['residual'] = rel_df['pred'] - rel_df['interactions_log'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df[rel_df['neighborhood_sim'] < 0.99]['residual'].plot(kind = 'hist', bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = dict(zip(dyn_comm_df[23].dropna().unique(),range(len(dyn_comm_df[23].dropna().unique()))))\n",
    "inverse_node_map = {node_map[k]:k for k in node_map}\n",
    "X = np.zeros((len(node_map),len(node_map)))\n",
    "# for edges, res in rel_df[rel_df['pred'] <=np.log(10)]['residual'].items():\n",
    "for edges, res in rel_df['residual'].items():\n",
    "    e1, e2 = dyn_comm_df[23].loc[edges[0]], dyn_comm_df[23].loc[edges[1]]\n",
    "    e1, e2 = node_map[e1], node_map[e2]\n",
    "    if e1 != e2:\n",
    "        X[e1,e2] = res\n",
    "        X[e2,e1] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = pd.Series(X.mean(axis=1).argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_df = pd.concat([pd.Series(X.mean(axis = 1)[mean_res.values]),mean_res], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_df['comm'] = mean_res_df[1].map(inverse_node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_to_front = dyn_comm_df[23].drop_duplicates().dropna().to_dict()\n",
    "front_to_dyn = {dyn_to_front[k]:k for k in dyn_to_front}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_comm_df[dyn_comm_df[23] == dyn_to_front[613]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_annotations(x, asc = False, n = 3):\n",
    "    if asc:\n",
    "        neighbours = X[x].argsort()[:n]\n",
    "    else:\n",
    "        neighbours = reversed(X[x].argsort()[-n:])\n",
    "    node_topic = annotate_community(inverse_node_map[x], n=3) \n",
    "    print(inverse_node_map[x], node_topic, community_year(23,inverse_node_map[x]))\n",
    "    neighbour_list = {0:node_topic}\n",
    "    for i,n in enumerate(neighbours):\n",
    "        edge = sorted([front_to_dyn[inverse_node_map[n]], front_to_dyn[inverse_node_map[x]]])\n",
    "        neighbour_topic = annotate_community(inverse_node_map[n])\n",
    "        print('\\t\\t', inverse_node_map[n], neighbour_topic, community_year(23,inverse_node_map[n]))\n",
    "        neighbour_list.update({i+1:neighbour_topic})\n",
    "    return neighbour_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_gaps = [edge_annotations(x, asc=False, n = 5) for x in mean_res_df.sort_values(0,ascending=False).iloc[:5][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([pd.Series(d) for d in knowledge_gaps]).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silos = pd.Series(X.sum(axis = 1)).sort_values().iloc[:10]\n",
    "silo_info = [{#'id':x,\n",
    "  'label':annotate_dynamic_community(front_to_dyn[x], reweighted=False, n_chars = 100, n_terms = 5),\n",
    "  # 'label_reweighted':annotate_dynamic_community(front_to_dyn[x], reweighted=True, n_chars = 100, n_terms = 5),\n",
    "  'size':len(comm_lookup(23, x)),\n",
    "  # 'z_score':total_interactions_z[x],\n",
    "  'degree': f'{deg_df[23][front_to_dyn[x]]:.0f}',\n",
    "  'density':f'{dens_df[23][front_to_dyn[x]]:.2f}'} for x in silos.index]\n",
    "silo_df = pd.DataFrame(silo_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silo_df.iloc[1:6].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
